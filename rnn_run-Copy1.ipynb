{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import tensorflow as tf \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt \n",
    "import random\n",
    "import shutil\n",
    "import tensorflow.contrib.learn as tflearn\n",
    "import tensorflow.contrib.layers as tflayers\n",
    "from tensorflow.contrib.learn.python.learn import learn_runner\n",
    "import tensorflow.contrib.metrics as metrics \n",
    "from tensorflow.contrib import rnn\n",
    "from sklearn.utils import shuffle\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'Data/Method 1 Upsampled/toRNN_Individual/data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CSV Data...\n",
      "('Number of Stable class examples: ', 2875)\n",
      "('Numbuer of Unstable class examples: ', 2875)\n"
     ]
    }
   ],
   "source": [
    "print('Reading CSV Data...')\n",
    "df = pd.read_csv(data)\n",
    "\n",
    "#Create a new feature for normal (non-fraudulent) transactions.\n",
    "df.loc[df.label == 0, 'Slip'] = 1\n",
    "df.loc[df.label == 1, 'Slip'] = 0\n",
    "\n",
    "\n",
    "#Rename 'Class' to 'Fraud'.\n",
    "df = df.rename(columns={'label': 'Stable'})\n",
    "\n",
    "df.Slip = df.Slip.astype(int)\n",
    "df.Stable = df.Stable.astype(int)\n",
    "\n",
    "\n",
    "Slip = df[df.Slip == 1]\n",
    "Stable = df[df.Stable == 1]\n",
    "\n",
    "#Equal number of Stable and Unstable class examples\n",
    "Stable = Stable.iloc[0:Slip.iloc[:,1].count(), :]\n",
    "\n",
    "print ('Number of Stable class examples: ', Stable.iloc[:,1].count())\n",
    "print ('Numbuer of Unstable class examples: ', Slip.iloc[:,1].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Length of train set', 4600)\n",
      "('Length of test set', 12150)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Add 80% of the slip transactions to X_train.\n",
    "X_train = Slip.sample(frac=0.8)\n",
    "count_Slip = len(X_train)\n",
    "\n",
    "# Add 80% of the stable transactions to X_train.\n",
    "X_train = pd.concat([X_train, Stable.sample(frac = 0.8)], axis = 0)\n",
    "\n",
    "# X_test contains all the transaction not in X_train.\n",
    "X_test = df.loc[~df.index.isin(X_train.index)]\n",
    "\n",
    "# Shuffle the dataframes so that the training is done in a random order.\n",
    "#X_train = shuffle(X_train)\n",
    "#X_test = shuffle(X_test)\n",
    "\n",
    "X_train = X_train.iloc[0:48600, :]\n",
    "X_test = X_test.iloc[0:12150, :]\n",
    "\n",
    "\n",
    "print ('Length of train set', len(X_train))\n",
    "print ('Length of test set', len (X_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add our target features to y_train and y_test.\n",
    "\n",
    "#df.loc[df.label == 0, 'Slip'] = 1\n",
    "\n",
    "y_train = pd.concat([X_train.Slip, X_train.Stable], axis=1)\n",
    "y_test = pd.concat([X_test.Slip, X_test.Stable], axis=1)\n",
    "\n",
    "# Drop target features from X_train and X_test.\n",
    "X_train = X_train.drop(['Slip', 'Stable'], axis = 1)\n",
    "X_test = X_test.drop(['Slip', 'Stable'], axis = 1)\n",
    "\n",
    "\n",
    "# Check to ensure all of the training/testing dataframes are of the correct length\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(loss_list, accuracy_list):\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.cla()\n",
    "    plt.plot(loss_list)\n",
    "    plt.plot(accuracy_list)\n",
    "    \n",
    "    #for batch_series_idx in range(batch_size):\n",
    "        #one_hot_output_series = np.array(predictions_series)[:, batch_series_idx, :]\n",
    "        #single_output_series = np.array([(1 if out[0] < 0.5 else 0) for out in one_hot_output_series])\n",
    "\n",
    "        #plt.subplot(2, 3, batch_series_idx + 2)\n",
    "        #plt.cla()\n",
    "        #plt.axis([0, truncated_backprop_length, 0, 2])\n",
    "        #left_offset = range(truncated_backprop_length)\n",
    "        #plt.bar(left_offset, batchX[batch_series_idx, :], width=1, color=\"blue\")\n",
    "        #plt.bar(left_offset, batchY[batch_series_idx, :] * 0.5, width=1, color=\"red\")\n",
    "        #plt.bar(left_offset, single_output_series * 0.3, width=1, color=\"green\")\n",
    "\n",
    "    plt.draw()\n",
    "    plt.pause(0.0001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "#unrolled features\n",
    "time_steps = 3 #83\n",
    "#LSTM units - features \n",
    "num_units=X_train.columns.size\n",
    "#number of exmples in batch\n",
    "n_input=10\n",
    "#learning rate for adam\n",
    "learning_rate=0.001\n",
    "n_classes=2\n",
    "#size of batch\n",
    "batch_size=30\n",
    "#truncated_backprop_length = 15\n",
    "total_series_length = len(X_train)\n",
    "\n",
    "\n",
    "num_batches = X_train.index.size//batch_size//n_input\n",
    "print(num_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weights and biases of appropriate shape to accomplish above task\n",
    "out_weights=tf.Variable(tf.random_normal([num_units,n_classes]))\n",
    "out_bias=tf.Variable(tf.random_normal([n_classes]))\n",
    "\n",
    "#defining placeholders\n",
    "#input image placeholder\n",
    "batchX_placeholder=tf.placeholder(\"float\",[None,time_steps,n_input])\n",
    "#input label placeholder\n",
    "batchY_placeholder=tf.placeholder(\"float\",[None,n_classes])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [],
   "source": [
    "#processing the input tensor from [batch_size,n_steps,n_input] to \"time_steps\" number of [batch_size,n_input] tensors\n",
    "input=tf.unstack(batchX_placeholder ,time_steps,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the network\n",
    "lstm_layer=rnn.BasicLSTMCell(num_units,forget_bias=1)\n",
    "outputs,_=rnn.static_rnn(lstm_layer,input,dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting last output of dimension [batch_size,num_units] to [batch_size,n_classes] by out_weight multiplication\n",
    "prediction=tf.matmul(outputs[-1],out_weights)+out_bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.reset_default_graph()\n",
    "\n",
    "#loss_function\n",
    "#loss = -tf.reduce_sum(batchY_placeholder * tf.log(prediction))\n",
    "loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction,labels=batchY_placeholder))\n",
    "output = tf.cast(prediction, tf.float32)\n",
    "labels = tf.cast(batchY_placeholder, tf.float32)\n",
    "\n",
    "\n",
    "#optimization\n",
    "opt=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "#model evaluation\n",
    "correct_prediction=tf.equal(tf.argmax(prediction,1),tf.argmax(batchY_placeholder,1))\n",
    "accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('For iter ', 10)\n",
      "('Training Accuracy ', 1.0)\n",
      "('Training Loss ', 0.05408721)\n",
      "__________________\n",
      "('For iter ', 20)\n",
      "('Training Accuracy ', 1.0)\n",
      "('Training Loss ', 0.0098836087)\n",
      "__________________\n",
      "('For iter ', 30)\n",
      "('Training Accuracy ', 1.0)\n",
      "('Training Loss ', 0.0049797562)\n",
      "__________________\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-578-ec4de80c9b95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             sess.run(opt, feed_dict={batchX_placeholder: batch_x, \n\u001b[0;32m---> 38\u001b[0;31m                                          batchY_placeholder: batch_y})\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ainur/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ainur/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ainur/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ainur/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ainur/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#from tensorflow.examples.tutorials.mnist import input_data\n",
    "#mnist=input_data.read_data_sets(\"/tmp/data/\",one_hot=True)\n",
    "\n",
    "#initialize variables\n",
    "\n",
    "init=tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(init)\n",
    "    #plt.ion()\n",
    "    #plt.figure()\n",
    "    #plt.show()\n",
    "    loss_list = []\n",
    "    acc_list = []\n",
    "    test_loss_list = []\n",
    "    test_acc_list = []\n",
    "    iter=1\n",
    "    while iter<800:\n",
    "        \n",
    "        for batch_idx in range(num_batches):\n",
    "            \n",
    "            #indexes for data\n",
    "            start_idx = batch_idx * batch_size \n",
    "            end_idx = start_idx + batch_size * n_input\n",
    "            batch_x = X_train.iloc[start_idx:end_idx, :]\n",
    "            batch_x = np.array(batch_x)\n",
    "            batch_x=batch_x.reshape((batch_size,time_steps,n_input))\n",
    "            \n",
    "            #TODO: replace with previous value\n",
    "            batch_x[np.isnan(batch_x)] = 0\n",
    "\n",
    "            #indexes for labels\n",
    "            start_idx = batch_idx * batch_size \n",
    "            end_idx = start_idx + batch_size\n",
    "            batch_y = y_train.iloc[start_idx:end_idx, :]\n",
    "\n",
    "            sess.run(opt, feed_dict={batchX_placeholder: batch_x, \n",
    "                                         batchY_placeholder: batch_y})\n",
    "            \n",
    "            \n",
    "            \n",
    "        if iter %10==0:\n",
    "\n",
    "            los=sess.run(loss, feed_dict={batchX_placeholder:batch_x,batchY_placeholder:batch_y})\n",
    "            acc=sess.run(accuracy, feed_dict={batchX_placeholder:batch_x,batchY_placeholder:batch_y})\n",
    "            \n",
    "            \n",
    "            loss_list.append(los)\n",
    "            acc_list.append(acc)\n",
    "            \n",
    "            #plot(loss_list, acc_list)\n",
    "            print(\"For iter \",iter)\n",
    "            print(\"Training Accuracy \",acc)\n",
    "            print(\"Training Loss \",los)\n",
    "            print(\"__________________\")\n",
    "\n",
    "        iter=iter+1\n",
    "        '''\n",
    "        for batch_idx in range(num_batches):\n",
    "\n",
    "            #indexes for data\n",
    "            start_idx = batch_idx * batch_size \n",
    "            end_idx = start_idx + batch_size * n_input\n",
    "            batch_x = X_test.iloc[start_idx:end_idx, :]\n",
    "            batch_x = np.array(batch_x)\n",
    "            batch_x=batch_x.reshape((batch_size,time_steps,n_input))\n",
    "\n",
    "            batch_x[np.isnan(batch_x)] = 0\n",
    "\n",
    "            #indexes for labels\n",
    "            start_idx = batch_idx * batch_size \n",
    "            end_idx = start_idx + batch_size\n",
    "            batch_y = y_test.iloc[start_idx:end_idx, :]\n",
    "\n",
    "            test_loss=sess.run(loss, feed_dict={batchX_placeholder:batch_x,batchY_placeholder:batch_y})\n",
    "            test_acc=sess.run(accuracy, feed_dict={batchX_placeholder:batch_x,batchY_placeholder:batch_y})\n",
    "            test_loss_list.append(test_loss)\n",
    "            test_acc_list.append(test_acc)\n",
    "            #plot(loss_list, acc_list)\n",
    "            print(\"For iter \",iter)\n",
    "            print(\"Testing Accuracy \",test_acc)\n",
    "            print(\"Testing Loss \",test_los)\n",
    "            print(\"__________________\")\n",
    "            '''\n",
    "\n",
    "            \n",
    "    #test_acc = sess.run(accuracy, feed_dict={batchX_placeholder: X_train, batchY_placeholder: y_train})\n",
    "    \n",
    "f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(10,4))\n",
    "\n",
    "ax1.plot(acc_list, color = 'green',  label = 'training') # green\n",
    "#ax1.plot(loss_list, color = 'orange', label = 'validation') # orange\n",
    "ax1.plot(test_acc, color = 'blue', label = 'testing') # blue\n",
    "ax1.set_title('Accuracy')\n",
    "\n",
    "\n",
    "ax2.plot(loss_list, color = 'green',  label = 'training')\n",
    "#ax2.plot(valid_cost_mean, color = 'orange', label = 'validation')\n",
    "ax2.plot(test_loss_list, color = 'blue', label = 'testing') \n",
    "ax2.set_title('Cost')\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=1, borderaxespad=0.)\n",
    "plt.xlabel('Epochs (x10)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
