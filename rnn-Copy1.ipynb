{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import tensorflow as tf \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt \n",
    "import random\n",
    "import shutil\n",
    "import tensorflow.contrib.learn as tflearn\n",
    "import tensorflow.contrib.layers as tflayers\n",
    "from tensorflow.contrib.learn.python.learn import learn_runner\n",
    "import tensorflow.contrib.metrics as metrics \n",
    "import tensorflow.contrib.rnn as rnn \n",
    "from sklearn.utils import shuffle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'DATASET/'\n",
    "TRAIN = 'traj_1_train_shuffled.csv'\n",
    "TEST = 'traj_1_test_shuffled.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Reading CSV Data...')\n",
    "df = pd.read_csv(PATH + TRAIN)\n",
    "test_df = pd.read_csv(PATH + TEST)\n",
    "\n",
    "#Create a new feature for normal (non-fraudulent) transactions.\n",
    "df.loc[df.label == 0, 'Slip'] = 1\n",
    "df.loc[df.label == 1, 'Slip'] = 0\n",
    "\n",
    "test_df.loc[test_df.label == 0, 'Slip'] = 1\n",
    "test_df.loc[test_df.label == 1, 'Slip'] = 0\n",
    "\n",
    "#Rename 'Class' to 'Fraud'.\n",
    "df = df.rename(columns={'label': 'Stable'})\n",
    "test_df = test_df.rename(columns={'label': 'Stable'})\n",
    "\n",
    "#Create dataframes of only Fraud and Normal transactions.\n",
    "Slip = df[df.Slip == 1]\n",
    "Stable = df[df.Stable == 1]\n",
    "\n",
    "test_Slip = test_df[test_df.Slip == 1]\n",
    "test_Stable = test_df[test_df.Stable == 1]\n",
    "\n",
    "# Set X_train equal to 80% of the fraudulent transactions.\n",
    "#X_train = Slip.sample\n",
    "#count_Slips = len(Slip)\n",
    "\n",
    "# Add 80% of the normal transactions to X_train.\n",
    "X_train = pd.concat([Slip, Stable], axis = 0)\n",
    "X_test = pd.concat([test_Slip, test_Stable], axis = 0)\n",
    "\n",
    "# X_test contains all the transaction not in X_train.\n",
    "#X_test = df.loc[~df.index.isin(X_train.index)]\n",
    "\n",
    "#Shuffle the dataframes so that the training is done in a random order.\n",
    "X_train = shuffle(X_train)\n",
    "X_test = shuffle(test_df)\n",
    "\n",
    "#Add our target features to y_train and y_test.\n",
    "y_train = X_train.Slip\n",
    "y_train = pd.concat([y_train, X_train.Stable], axis=1)\n",
    "\n",
    "y_test = X_test.Slip\n",
    "y_test = pd.concat([y_test, X_test.Stable], axis=1)\n",
    "\n",
    "#Drop target features from X_train and X_test.\n",
    "X_train = X_train.drop(['Slip','Stable'], axis = 1)\n",
    "X_test = X_test.drop(['Slip','Stable'], axis = 1)\n",
    "\n",
    "\n",
    "#Select certain features\n",
    "#cols = [c for c in X_train.columns if c.lower()[:5] != 'median']\n",
    "X_train = X_train[X_train.columns.drop(list(X_train.filter(regex='left')))]\n",
    "X_test = X_test[X_test.columns.drop(list(X_test.filter(regex='left')))]\n",
    "\n",
    "#X_train = X_train[X_train.columns.drop(list(X_train.filter(regex='right')))]\n",
    "#X_test = X_test[X_test.columns.drop(list(X_test.filter(regex='right')))]\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "\n",
    "#X_train = X_train[:, 3]\n",
    "#y_train = y_train[:, 1]\n",
    "\n",
    "#X_test = X_test[:, 3]\n",
    "#y_test = y_test [:, 1]\n",
    "print (X_train.shape)\n",
    "print (y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "training_epochs = 1000\n",
    "batch_size = 22\n",
    "display_step = 20\n",
    "\n",
    "# Network Parameters\n",
    "num_input = X_train[:, 3].size\n",
    "print (num_input)\n",
    "timesteps = 1\n",
    "num_hidden = int(X_train[0, :].size * .5)\n",
    "num_classes = 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, timesteps, num_input])\n",
    "Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([num_hidden, num_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.reset_default_graph()\n",
    "\n",
    "\n",
    "def RNN(x, weights, biases):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, timesteps, n_input)\n",
    "    # Required shape: 'timesteps' tensors list of shape (batch_size, n_input)\n",
    "\n",
    "    # Unstack to get a list of 'timesteps' tensors of shape (batch_size, n_input)\n",
    "    x = tf.unstack(x, timesteps, 1)\n",
    "\n",
    "    # Define a lstm cell with tensorflow\n",
    "    lstm_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n",
    "\n",
    "    # Get lstm cell output\n",
    "    with tf.variable_scope('lstm1'):\n",
    "        outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "\n",
    "logits = RNN(X, weights, biases)\n",
    "prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model (with test logits, for dropout to be disabled)\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(1, training_epochs+1):\n",
    "        \n",
    "        for batch in range(int(num_input/batch_size)):\n",
    "            batch_x = X_train[batch*batch_size : (1+batch)*batch_size]\n",
    "            batch_y = y_train[batch*batch_size : (1+batch)*batch_size]\n",
    "            \n",
    "            batch_x = batch_x.reshape((batch_size, timesteps, num_input))\n",
    "            #print (\"Batch shape\", batch_x.shape)\n",
    "            # Run optimization op (backprop)\n",
    "            sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
    "            \n",
    "            if epoch % display_step == 0 or epoch == 1:\n",
    "                # Calculate batch loss and accuracy\n",
    "                loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,\n",
    "                                                                     Y: batch_y})\n",
    "                #print(\"Step \" + str(epoch) + \", Minibatch Loss= \" + \\\n",
    "                 #     \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  #    \"{:.3f}\".format(acc))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy for test data\n",
    "    test_len = 22\n",
    "    test_data = X_test[:test_len].reshape((-1, timesteps, num_input))\n",
    "    test_label = y_test[:test_len]\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={X: test_data, Y: test_label}))\n",
    "    \n",
    "    print (prediction.eval(feed_dict = {X: test_data, Y: test_label}))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig = dict(data=data, layout=layout)\n",
    "\n",
    "#py.iplot(fig, filename='Sine Wave Slider')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
